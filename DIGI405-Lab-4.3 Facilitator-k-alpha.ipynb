{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/karinstahel/b3624de8953a0e3773b53c8ef9ddc638/digi405-lab-4-3-facilitator-k-alpha.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff2df38-60a3-4e8d-b41e-1e05c3ab2e43",
      "metadata": {
        "id": "cff2df38-60a3-4e8d-b41e-1e05c3ab2e43"
      },
      "source": [
        "# DIGI405 Lab 4.3: Facilitator Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bb94781-628e-473f-928c-23b8296c14a6",
      "metadata": {
        "id": "3bb94781-628e-473f-928c-23b8296c14a6"
      },
      "source": [
        "This notebook can be used to collate the class annotations, measure agreement with Krippendorff's Alpha, and find the texts with the lowest and highest average confidence, and the greatest confidence range.\n",
        "\n",
        "You don't need to change any code apart from adding the zip file path.\n",
        "\n",
        "1. Upload a zip file of the labelled CSV files from the class\n",
        "2. Enter the path to the zip as the `zip_file_path` variable\n",
        "3. Run the cells - do a visual check of output to make sure it is as expected\n",
        "4. Share the `=== CONFIDENCE ANALYSIS PER TEXT ===` results with the class\n",
        "5. Share the `Krippendorff's alpha:` score with the class and the `encoded_values.csv` which is in a format that students can upload directly to the online K-Alpha calcuator (test this first - the result using the 'Nominal' data type option should be the same as calculated here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e373ed09-409d-46a4-9c17-1177ac8e8170",
      "metadata": {
        "id": "e373ed09-409d-46a4-9c17-1177ac8e8170"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import re\n",
        "import io\n",
        "from typing import Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f62832-fe62-4c64-9244-bb6a876b2664",
      "metadata": {
        "id": "14f62832-fe62-4c64-9244-bb6a876b2664"
      },
      "outputs": [],
      "source": [
        "zip_file_path = \"df_labelled_test.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20302ba-9922-410f-97ff-90c0b63af0a2",
      "metadata": {
        "id": "a20302ba-9922-410f-97ff-90c0b63af0a2"
      },
      "outputs": [],
      "source": [
        "def process_csv_zip(zip_file_path):\n",
        "    \"\"\"\n",
        "    Given multiple CSV files in a zip archive, concatenate into a single dataframe.\n",
        "\n",
        "    This function processes CSV files matching the pattern \"df_labelled_*.csv\" from\n",
        "    a zip archive. It merges them horizontally based on the \"text\" column, renames\n",
        "    the \"label\" and \"confidence\" columns with suffixes based on the filename, and\n",
        "    adds encoded columns that convert text labels to integers.\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "        csv_files = [f for f in zip_ref.namelist() if f.startswith(\"df_labelled_\") and f.endswith(\".csv\")]\n",
        "\n",
        "        if not csv_files:\n",
        "            print(\"No matching CSV files found in the zip archive.\")\n",
        "            return None\n",
        "\n",
        "        first_file = csv_files[0]\n",
        "        match = re.search(r\"df_labelled_(.+)\\.csv\", first_file)\n",
        "        first_suffix = match.group(1) if match else \"unknown\"\n",
        "\n",
        "        with zip_ref.open(first_file) as f:\n",
        "            result_df = pd.read_csv(io.TextIOWrapper(f, encoding = \"utf-8\"))\n",
        "\n",
        "        # Rename columns in first dataframe\n",
        "        result_df = result_df.rename(columns = {\n",
        "            \"label\": f\"label_{first_suffix}\",\n",
        "            \"confidence\": f\"confidence_{first_suffix}\"\n",
        "        })\n",
        "\n",
        "        # Create mapping dictionary for label encoding\n",
        "        label_mapping = {\"NEGATIVE\": 0, \"NEUTRAL\": 1, \"POSITIVE\": 2}\n",
        "\n",
        "        # Create encoded column for first dataframe\n",
        "        result_df[f\"encoded_{first_suffix}\"] = result_df[f\"label_{first_suffix}\"].map(label_mapping)\n",
        "\n",
        "        # Process remaining CSV files\n",
        "        for file in csv_files[1:]:\n",
        "            # Extract suffix from filename\n",
        "            match = re.search(r\"df_labelled_(.+)\\.csv\", file)\n",
        "            if match:\n",
        "                suffix = match.group(1)\n",
        "\n",
        "                with zip_ref.open(file) as f:\n",
        "                    df = pd.read_csv(io.TextIOWrapper(f, encoding = \"utf-8\"))\n",
        "\n",
        "                # Create temporary df with renamed columns\n",
        "                temp_df = df[[\"text\", \"label\", \"confidence\"]].rename(columns={\n",
        "                    \"label\": f\"label_{suffix}\",\n",
        "                    \"confidence\": f\"confidence_{suffix}\"\n",
        "                })\n",
        "\n",
        "                # Create encoded column\n",
        "                temp_df[f\"encoded_{suffix}\"] = temp_df[f\"label_{suffix}\"].map(label_mapping)\n",
        "\n",
        "                # Merge with result dataframe\n",
        "                result_df = pd.merge(result_df, temp_df, on = \"text\", how = \"outer\")\n",
        "\n",
        "        result_df = result_df.reset_index(drop=True)\n",
        "\n",
        "        return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43cd187d-fe81-4d10-bfcf-0f449205714c",
      "metadata": {
        "id": "43cd187d-fe81-4d10-bfcf-0f449205714c"
      },
      "outputs": [],
      "source": [
        "merged_df = process_csv_zip(zip_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8be7cc9c-2693-4dc9-a41e-592622331ef3",
      "metadata": {
        "id": "8be7cc9c-2693-4dc9-a41e-592622331ef3"
      },
      "outputs": [],
      "source": [
        "# Check the resulting df\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "512228bd-b576-4400-8822-cb2196527a0d",
      "metadata": {
        "id": "512228bd-b576-4400-8822-cb2196527a0d"
      },
      "outputs": [],
      "source": [
        "def analyse_confidence_per_text(merged_df):\n",
        "    \"\"\"\n",
        "    Get the average, min, max, and range of\n",
        "    confidence per text.\n",
        "\n",
        "    Print results and return as a dict.\n",
        "    \"\"\"\n",
        "    df = merged_df.copy()\n",
        "    confidence_cols = [col for col in df.columns if \"confidence_\" in col]\n",
        "\n",
        "    if not confidence_cols:\n",
        "        print(\"No confidence columns found in the dataframe\")\n",
        "        return None\n",
        "\n",
        "    for col in confidence_cols:\n",
        "        if df[col].dtype.kind not in \"iuf\":  # if not integer, unsigned int, or float\n",
        "            df[col] = df[col].astype(str).str.rstrip(\"%\").astype(int)\n",
        "\n",
        "    # Calculate statistics per text\n",
        "    df[\"avg_confidence\"] = df[confidence_cols].mean(axis = 1)\n",
        "    df[\"min_confidence\"] = df[confidence_cols].min(axis = 1)\n",
        "    df[\"max_confidence\"] = df[confidence_cols].max(axis = 1)\n",
        "    df[\"range_confidence\"] = df[\"max_confidence\"] - df[\"min_confidence\"]\n",
        "\n",
        "    highest_avg_idx = df[\"avg_confidence\"].idxmax()\n",
        "    lowest_avg_idx = df[\"avg_confidence\"].idxmin()\n",
        "    highest_range_idx = df[\"range_confidence\"].idxmax()\n",
        "\n",
        "    # Create result dictionary\n",
        "    results = {\n",
        "        \"highest_avg_confidence\": {\n",
        "            \"text\": df.loc[highest_avg_idx, \"text\"],\n",
        "            \"average\": df.loc[highest_avg_idx, \"avg_confidence\"],\n",
        "        },\n",
        "        \"lowest_avg_confidence\": {\n",
        "            \"text\": df.loc[lowest_avg_idx, \"text\"],\n",
        "            \"average\": df.loc[lowest_avg_idx, \"avg_confidence\"],\n",
        "        },\n",
        "        \"highest_range\": {\n",
        "            \"text\": df.loc[highest_range_idx, \"text\"],\n",
        "            \"range\": df.loc[highest_range_idx, \"range_confidence\"],\n",
        "            \"min\": df.loc[highest_range_idx, \"min_confidence\"],\n",
        "            \"max\": df.loc[highest_range_idx, \"max_confidence\"],\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Print results in a readable format\n",
        "    print(\"\\n=== CONFIDENCE ANALYSIS PER TEXT ===\\n\")\n",
        "\n",
        "    print(\"TEXT WITH HIGHEST AVERAGE CONFIDENCE:\")\n",
        "    print(f\"Text: \\'{results[\"highest_avg_confidence\"][\"text\"]}\\'\")\n",
        "    print(f\"Average confidence: {results[\"highest_avg_confidence\"][\"average\"]:.2f}\")\n",
        "    print()\n",
        "\n",
        "    print(\"TEXT WITH LOWEST AVERAGE CONFIDENCE:\")\n",
        "    print(f\"Text: \\'{results[\"lowest_avg_confidence\"][\"text\"]}\\'\")\n",
        "    print(f\"Average confidence: {results[\"lowest_avg_confidence\"][\"average\"]:.2f}\")\n",
        "    print()\n",
        "\n",
        "    print(\"TEXT WITH BIGGEST CONFIDENCE RANGE:\")\n",
        "    print(f\"Text: \\'{results[\"highest_range\"][\"text\"]}\\'\")\n",
        "    print(f\"Range: {results[\"highest_range\"][\"range\"]} (from {results[\"highest_range\"][\"min\"]} to {results[\"highest_range\"][\"max\"]})\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd6a3c0-bff1-43bc-8a44-8c32ca7f0ba2",
      "metadata": {
        "id": "abd6a3c0-bff1-43bc-8a44-8c32ca7f0ba2"
      },
      "outputs": [],
      "source": [
        "results = analyse_confidence_per_text(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a1ff541-f3c2-4e32-a9a2-dac15401f935",
      "metadata": {
        "id": "5a1ff541-f3c2-4e32-a9a2-dac15401f935"
      },
      "outputs": [],
      "source": [
        "# Calculate Krippendorff's alpha\n",
        "# Developed with help from Claude 3.7 Sonnet\n",
        "# Using method 'C. Nominal data, any number of observers, missing data' from:\n",
        "# Krippendorff, K. (2011, January 25). Computing Krippendorffâ€™s Alpha-Reliability. https://repository.upenn.edu/handle/20.500.14332/2089\n",
        "\n",
        "def calculate_krippendorff_alpha(data: Union[pd.DataFrame, np.ndarray], verbose=True) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Krippendorff's alpha for nominal data.\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame or array where rows are units and columns are observers\n",
        "        verbose: Whether to print detailed information about the calculation\n",
        "\n",
        "    Returns:\n",
        "        Krippendorff's alpha coefficient\n",
        "    \"\"\"\n",
        "    # Convert to numpy array with NaN for missing values\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data = data.values\n",
        "    data = np.where(data == 'NA', np.nan, data).astype(float)\n",
        "\n",
        "    # Initialise coincidence matrix as dictionary\n",
        "    coincidences = {}\n",
        "    value_counts = {}\n",
        "    total_coincidences = 0\n",
        "    total_values = 0\n",
        "    total_pairs = 0  # Track the actual number of pairs\n",
        "\n",
        "    # For each unit (row)\n",
        "    for unit in data:\n",
        "        # Get valid values in this unit\n",
        "        valid_values = unit[~np.isnan(unit)]\n",
        "        mu = len(valid_values)\n",
        "\n",
        "        if mu <= 1:\n",
        "            continue  # Skip units with 0 or 1 valid value\n",
        "\n",
        "        # Add to total values that can be paired\n",
        "        total_values += mu\n",
        "\n",
        "        # Add to total pairs within this unit\n",
        "        unit_pairs = mu * (mu - 1)\n",
        "        total_pairs += unit_pairs\n",
        "\n",
        "        # Calculate all coincidences within this unit\n",
        "        for i, val1 in enumerate(valid_values):\n",
        "            for j, val2 in enumerate(valid_values):\n",
        "                if i != j:  # Don't pair a value with itself\n",
        "                    # Create pair key for coincidence matrix\n",
        "                    pair = (val1, val2)\n",
        "\n",
        "                    # Add to coincidence matrix with weight 1/(mu-1)\n",
        "                    coincidences[pair] = coincidences.get(pair, 0) + 1/(mu-1)\n",
        "\n",
        "                    # Update value count for val1\n",
        "                    value_counts[val1] = value_counts.get(val1, 0) + 1/(mu-1)\n",
        "\n",
        "                    # Add to total coincidences\n",
        "                    total_coincidences += 1/(mu-1)\n",
        "\n",
        "    # Calculate observed disagreement (Do)\n",
        "    Do = 0\n",
        "    for (val1, val2), count in coincidences.items():\n",
        "        if val1 != val2:  # For nominal data, disagreement when values differ\n",
        "            Do += count\n",
        "\n",
        "    # Calculate expected disagreement (De)\n",
        "    De = 0\n",
        "    for val1, count1 in value_counts.items():\n",
        "        for val2, count2 in value_counts.items():\n",
        "            if val1 != val2:\n",
        "                De += (count1 * count2) / (total_coincidences - 1)\n",
        "\n",
        "    # Calculate alpha\n",
        "    alpha = 1 - (Do / De)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Total pairable values: {total_values}\")\n",
        "        print(f\"Total pairs compared: {total_pairs}\")\n",
        "        print(f\"Total coincidences (n): {total_coincidences}\")\n",
        "        print(f\"Value counts (n.c): {value_counts}\")\n",
        "        print(f\"Observed disagreement (Do): {Do}\")\n",
        "        print(f\"Expected disagreement (De): {De}\")\n",
        "        print(f\"Krippendorff's alpha: {alpha:.3f}\")\n",
        "\n",
        "    return alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d895eae-884d-4cda-84c0-10fec76ad9f1",
      "metadata": {
        "id": "7d895eae-884d-4cda-84c0-10fec76ad9f1"
      },
      "outputs": [],
      "source": [
        "# Extract just the encoded columns from the merged dataframe\n",
        "encoded_cols = [col for col in merged_df.columns if \"encoded_\" in col]\n",
        "encoded_df = merged_df[encoded_cols]\n",
        "\n",
        "encoded_df = encoded_df.fillna(\"NA\")\n",
        "\n",
        "print(f\"Calculating Krippendorff's alpha for {len(encoded_cols)} encoded columns\")\n",
        "print(f\"Number of texts analysed: {len(encoded_df)}\")\n",
        "\n",
        "alpha = calculate_krippendorff_alpha(encoded_df, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d274941-c541-4174-beb0-69e2bc718d58",
      "metadata": {
        "id": "3d274941-c541-4174-beb0-69e2bc718d58"
      },
      "outputs": [],
      "source": [
        "# encoded_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18fc8257-66a2-4400-bb31-5fd1ec483e77",
      "metadata": {
        "id": "18fc8257-66a2-4400-bb31-5fd1ec483e77"
      },
      "outputs": [],
      "source": [
        "encoded_df.to_csv(\"encoded_values.csv\", header=False, index=False)\n",
        "print(\"Saved encoded values to encoded_values.csv (no headers, no index)\\nDownload this file and share with the class for testing with online K-Alpha calculator\\nPlease test first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65db0f11-796c-4222-bba2-98f7c7fac862",
      "metadata": {
        "id": "65db0f11-796c-4222-bba2-98f7c7fac862"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.12 (ipykernel)",
      "language": "python",
      "name": "python3.12"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
