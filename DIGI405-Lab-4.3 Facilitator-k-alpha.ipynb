{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cff2df38-60a3-4e8d-b41e-1e05c3ab2e43",
      "metadata": {
        "id": "cff2df38-60a3-4e8d-b41e-1e05c3ab2e43"
      },
      "source": [
        "# DIGI405 Lab 4.3: Facilitator Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bb94781-628e-473f-928c-23b8296c14a6",
      "metadata": {
        "id": "3bb94781-628e-473f-928c-23b8296c14a6"
      },
      "source": [
        "This notebook can be used by the class facilitator or tutor to collate the class annotations, measure agreement with Krippendorff's Alpha, and find the texts with the lowest and highest average confidence, and the greatest confidence range.\n",
        "\n",
        "You don't need to change any code apart from adding the zip file path.\n",
        "\n",
        "1. Upload this notebook to JupyterHub along with the zip file of labelled CSV files from the class\n",
        "2. Enter the path to the zip as the `zip_file_path` variable\n",
        "3. Run the cells - do a visual check of output to make sure it is as expected\n",
        "4. Share the `=== CONFIDENCE ANALYSIS PER TEXT ===` results and the box plot with the class\n",
        "5. Share the `Krippendorff's alpha:` score with the class and the `encoded_annotations.csv` which is in a format that students can upload directly to the online K-Alpha calculator (test this first - the result using the 'Nominal' data type option should be the same as calculated here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e373ed09-409d-46a4-9c17-1177ac8e8170",
      "metadata": {
        "id": "e373ed09-409d-46a4-9c17-1177ac8e8170"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import re\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f62832-fe62-4c64-9244-bb6a876b2664",
      "metadata": {
        "id": "14f62832-fe62-4c64-9244-bb6a876b2664"
      },
      "outputs": [],
      "source": [
        "zip_file_path = \"df_labelled_test.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20302ba-9922-410f-97ff-90c0b63af0a2",
      "metadata": {
        "id": "a20302ba-9922-410f-97ff-90c0b63af0a2"
      },
      "outputs": [],
      "source": [
        "def process_csv_zip(zip_file_path):\n",
        "    \"\"\"\n",
        "    Given multiple CSV files in a zip archive, concatenate into a single dataframe.\n",
        "\n",
        "    This function processes CSV files containing the pattern \"*df_labelled_*.csv\" from\n",
        "    a zip archive. It merges them horizontally based on the \"text\" column, renames\n",
        "    the \"label\" and \"confidence\" columns with suffixes based on the filename, and\n",
        "    adds encoded columns that convert text labels to integers.\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "        csv_files = [f for f in zip_ref.namelist() if \"df_labelled_\" in f and f.endswith(\".csv\")]\n",
        "\n",
        "        if not csv_files:\n",
        "            print(\"No matching CSV files found in the zip archive.\")\n",
        "            return None\n",
        "\n",
        "        first_file = csv_files[0]\n",
        "        match = re.search(r\"df_labelled_([^\\.]+)\\.csv\", first_file)\n",
        "        first_suffix = match.group(1) if match else \"unknown\"\n",
        "\n",
        "        with zip_ref.open(first_file) as f:\n",
        "            result_df = pd.read_csv(io.TextIOWrapper(f, encoding = \"utf-8\"))\n",
        "\n",
        "        # Store original text order\n",
        "        original_text_order = result_df[\"text\"].tolist()\n",
        "\n",
        "        # Rename columns in first dataframe\n",
        "        result_df = result_df.rename(columns = {\n",
        "            \"label\": f\"label_{first_suffix}\",\n",
        "            \"confidence\": f\"confidence_{first_suffix}\"\n",
        "        })\n",
        "\n",
        "        # Create mapping dictionary for label encoding\n",
        "        label_mapping = {\"NEGATIVE\": 0, \"NEUTRAL\": 1, \"POSITIVE\": 2}\n",
        "\n",
        "        # Create encoded column for first dataframe\n",
        "        result_df[f\"encoded_{first_suffix}\"] = result_df[f\"label_{first_suffix}\"].map(label_mapping)\n",
        "\n",
        "        # Process remaining CSV files\n",
        "        for file in csv_files[1:]:\n",
        "            # Extract suffix from filename\n",
        "            match = re.search(r\"df_labelled_([^\\.]+)\\.csv\", file)\n",
        "            if match:\n",
        "                suffix = match.group(1)\n",
        "\n",
        "                with zip_ref.open(file) as f:\n",
        "                    df = pd.read_csv(io.TextIOWrapper(f, encoding = \"utf-8\"))\n",
        "\n",
        "                # Create temporary df with renamed columns\n",
        "                temp_df = df[[\"text\",\n",
        "                              \"label\",\n",
        "                              \"confidence\"]].rename(columns={\n",
        "                                  \"label\": f\"label_{suffix}\",\n",
        "                                  \"confidence\": f\"confidence_{suffix}\"\n",
        "                                  })\n",
        "\n",
        "                # Create encoded column\n",
        "                temp_df[f\"encoded_{suffix}\"] = temp_df[f\"label_{suffix}\"].map(label_mapping)\n",
        "\n",
        "                # Merge with result dataframe\n",
        "                result_df = pd.merge(result_df, temp_df, on = \"text\", how = \"outer\")\n",
        "\n",
        "        # Restore original row order (for texts present in the first file)\n",
        "        result_df[\"_original_order\"] = result_df[\"text\"].apply(\n",
        "            lambda x: original_text_order.index(x) if x in original_text_order else len(original_text_order)\n",
        "        )\n",
        "\n",
        "        # Sort by original order, then put new texts (not in first file) at the end\n",
        "        result_df = result_df.sort_values(\"_original_order\").drop(columns=[\"_original_order\"])\n",
        "\n",
        "        result_df = result_df.reset_index(drop=True)\n",
        "\n",
        "        confidence_cols = [col for col in result_df.columns if \"confidence_\" in col]\n",
        "\n",
        "        for col in confidence_cols:\n",
        "            result_df[col] = (\n",
        "            result_df[col]\n",
        "            .astype(str)\n",
        "            .str.rstrip(\"%\")\n",
        "            .replace(\"nan\", pd.NA)\n",
        "            .astype(\"Int64\")  # Capital I, allows NA\n",
        "            )\n",
        "\n",
        "        result_df[\"tweet_id\"] = [f\"Tweet {i+1}\" for i in range(len( result_df))]\n",
        "\n",
        "        return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43cd187d-fe81-4d10-bfcf-0f449205714c",
      "metadata": {
        "id": "43cd187d-fe81-4d10-bfcf-0f449205714c"
      },
      "outputs": [],
      "source": [
        "merged_df = process_csv_zip(zip_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8be7cc9c-2693-4dc9-a41e-592622331ef3",
      "metadata": {
        "id": "8be7cc9c-2693-4dc9-a41e-592622331ef3"
      },
      "outputs": [],
      "source": [
        "# Check the resulting df\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "512228bd-b576-4400-8822-cb2196527a0d",
      "metadata": {
        "id": "512228bd-b576-4400-8822-cb2196527a0d"
      },
      "outputs": [],
      "source": [
        "def analyse_confidence_per_text(df):\n",
        "    \"\"\"\n",
        "    Get the average, min, max, and interquartile range of\n",
        "    confidence per text.\n",
        "    \"\"\"\n",
        "    confidence_cols = [col for col in df.columns if \"confidence_\" in col]\n",
        "\n",
        "    df[\"avg_confidence\"] = df[confidence_cols].mean(axis=1)\n",
        "    df[\"min_confidence\"] = df[confidence_cols].min(axis=1)\n",
        "    df[\"max_confidence\"] = df[confidence_cols].max(axis=1)\n",
        "    df[\"q1_confidence\"] = df[confidence_cols].quantile(0.25, axis=1)\n",
        "    df[\"q3_confidence\"] = df[confidence_cols].quantile(0.75, axis=1)\n",
        "    df[\"iqr_confidence\"] = df[\"q3_confidence\"] - df[\"q1_confidence\"]\n",
        "    df[\"range_confidence\"] = df[\"max_confidence\"] - df[\"min_confidence\"]\n",
        "\n",
        "    # Find the maximum/minimum values\n",
        "    highest_avg = df[\"avg_confidence\"].max()\n",
        "    lowest_avg = df[\"avg_confidence\"].min()\n",
        "    highest_iqr = df[\"iqr_confidence\"].max()\n",
        "\n",
        "    # Find all tweets with these values\n",
        "    highest_avg_tweets = df[df[\"avg_confidence\"] == highest_avg]\n",
        "    lowest_avg_tweets = df[df[\"avg_confidence\"] == lowest_avg]\n",
        "    highest_iqr_tweets = df[df[\"iqr_confidence\"] == highest_iqr]\n",
        "\n",
        "    # Print results including all ties\n",
        "    print(\"\\n=== CONFIDENCE ANALYSIS PER TEXT ===\\n\")\n",
        "\n",
        "    print(f\"TEXT(S) WITH HIGHEST AVERAGE CONFIDENCE ({highest_avg:.2f}):\")\n",
        "    for _, row in highest_avg_tweets.iterrows():\n",
        "        print(f\"{row['tweet_id']}: '{row['text']}'\")\n",
        "    print()\n",
        "\n",
        "    print(f\"TEXT(S) WITH LOWEST AVERAGE CONFIDENCE ({lowest_avg:.2f}):\")\n",
        "    for _, row in lowest_avg_tweets.iterrows():\n",
        "        print(f\"{row['tweet_id']}: '{row['text']}'\")\n",
        "    print()\n",
        "\n",
        "    print(f\"TEXT(S) WITH BIGGEST CONFIDENCE INTERQUARTILE RANGE (IQR = {highest_iqr:.2f}):\")\n",
        "    for _, row in highest_iqr_tweets.iterrows():\n",
        "        print(f\"{row['tweet_id']}: '{row['text']}'\")\n",
        "        print(f\"  Q1: {row['q1_confidence']:.2f}, Q3: {row['q3_confidence']:.2f}, Range: {row['range_confidence']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd6a3c0-bff1-43bc-8a44-8c32ca7f0ba2",
      "metadata": {
        "id": "abd6a3c0-bff1-43bc-8a44-8c32ca7f0ba2"
      },
      "outputs": [],
      "source": [
        "analyse_confidence_per_text(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f320e4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confidence_boxplots(merged_df):\n",
        "    \"\"\"\n",
        "    Create boxplots of confidence scores for each tweet.\n",
        "    \"\"\"\n",
        "    confidence_cols = [col for col in merged_df.columns if \"confidence_\" in col]\n",
        "\n",
        "    df = merged_df.copy()\n",
        "    df_long = pd.melt(\n",
        "        df,\n",
        "        id_vars=[\"tweet_id\"],\n",
        "        value_vars = confidence_cols,\n",
        "        var_name = \"Annotator\",\n",
        "        value_name = \"Confidence\"\n",
        "    )\n",
        "\n",
        "    df_long[\"Annotator\"] = df_long[\"Annotator\"].str.replace(\"confidence_\", \"\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    sns.boxplot(\n",
        "        data = df_long,\n",
        "        x = \"tweet_id\",\n",
        "        y = \"Confidence\",\n",
        "        ax = ax,\n",
        "        color = \"royalblue\"\n",
        "    )\n",
        "\n",
        "    plt.title(\"Confidence Scores: Distribution by Tweet\", fontsize = 10)\n",
        "    plt.ylabel(\"Confidence Score\", fontsize = 9)\n",
        "    # plt.xlabel(\"Tweet Number\", fontsize = 9)\n",
        "    plt.xticks(rotation = 90, ha = \"center\", fontsize = 9)\n",
        "    plt.yticks(fontsize = 9)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f6e4d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plot_confidence_boxplots(merged_df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a1ff541-f3c2-4e32-a9a2-dac15401f935",
      "metadata": {
        "id": "5a1ff541-f3c2-4e32-a9a2-dac15401f935"
      },
      "outputs": [],
      "source": [
        "# Calculate Krippendorff's alpha\n",
        "# Developed with help from Claude 3.7 Sonnet\n",
        "# Using method 'C. Nominal data, any number of observers, missing data' from:\n",
        "# Krippendorff, K. (2011, January 25). Computing Krippendorffâ€™s Alpha-Reliability. https://repository.upenn.edu/handle/20.500.14332/2089\n",
        "\n",
        "def calculate_krippendorff_alpha(data: Union[pd.DataFrame, np.ndarray], verbose=True) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Krippendorff's alpha for nominal data.\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame or array where rows are units and columns are observers\n",
        "        verbose: Whether to print detailed information about the calculation\n",
        "\n",
        "    Returns:\n",
        "        Krippendorff's alpha coefficient\n",
        "    \"\"\"\n",
        "    # Convert to numpy array with NaN for missing values\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data = data.to_numpy(dtype=\"float64\", na_value=np.nan)\n",
        "    else:\n",
        "        data = np.array(data, dtype=\"float64\")\n",
        "\n",
        "    # Initialise coincidence matrix as dictionary\n",
        "    coincidences = {}\n",
        "    value_counts = {}\n",
        "    total_coincidences = 0\n",
        "    total_values = 0\n",
        "    total_pairs = 0  # Track the actual number of pairs\n",
        "\n",
        "    # For each unit (row)\n",
        "    for unit in data:\n",
        "        # Get valid values in this unit\n",
        "        valid_values = unit[~np.isnan(unit)]\n",
        "        mu = len(valid_values)\n",
        "\n",
        "        if mu <= 1:\n",
        "            continue  # Skip units with 0 or 1 valid value\n",
        "\n",
        "        # Add to total values that can be paired\n",
        "        total_values += mu\n",
        "\n",
        "        # Add to total pairs within this unit\n",
        "        unit_pairs = mu * (mu - 1)\n",
        "        total_pairs += unit_pairs\n",
        "\n",
        "        # Calculate all coincidences within this unit\n",
        "        for i, val1 in enumerate(valid_values):\n",
        "            for j, val2 in enumerate(valid_values):\n",
        "                if i != j:  # Don't pair a value with itself\n",
        "                    # Create pair key for coincidence matrix\n",
        "                    pair = (val1, val2)\n",
        "\n",
        "                    # Add to coincidence matrix with weight 1/(mu-1)\n",
        "                    coincidences[pair] = coincidences.get(pair, 0) + 1/(mu-1)\n",
        "\n",
        "                    # Update value count for val1\n",
        "                    value_counts[val1] = value_counts.get(val1, 0) + 1/(mu-1)\n",
        "\n",
        "                    # Add to total coincidences\n",
        "                    total_coincidences += 1/(mu-1)\n",
        "\n",
        "    # Calculate observed disagreement (Do)\n",
        "    Do = 0\n",
        "    for (val1, val2), count in coincidences.items():\n",
        "        if val1 != val2:  # For nominal data, disagreement when values differ\n",
        "            Do += count\n",
        "\n",
        "    # Calculate expected disagreement (De)\n",
        "    De = 0\n",
        "    for val1, count1 in value_counts.items():\n",
        "        for val2, count2 in value_counts.items():\n",
        "            if val1 != val2:\n",
        "                De += (count1 * count2) / (total_coincidences - 1)\n",
        "\n",
        "    # Calculate alpha\n",
        "    alpha = 1 - (Do / De)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Total pairable values: {total_values}\")\n",
        "        print(f\"Total pairs compared: {total_pairs}\")\n",
        "        # print(f\"Total coincidences (n): {total_coincidences}\")\n",
        "        # print(f\"Value counts (n.c): {value_counts}\")\n",
        "        # print(f\"Observed disagreement (Do): {Do}\")\n",
        "        # print(f\"Expected disagreement (De): {De}\")\n",
        "        print(f\"Krippendorff's alpha: {alpha:.3f}\")\n",
        "\n",
        "    return alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d895eae-884d-4cda-84c0-10fec76ad9f1",
      "metadata": {
        "id": "7d895eae-884d-4cda-84c0-10fec76ad9f1"
      },
      "outputs": [],
      "source": [
        "confidence_cols = [col for col in merged_df.columns if \"confidence_\" in col]\n",
        "encoded_cols = [col for col in merged_df.columns if \"encoded_\" in col]\n",
        "encoded_df = merged_df[encoded_cols].copy()\n",
        "\n",
        "# For each annotator, check their confidence scores\n",
        "# Replace encoded values with NA where confidence is 0\n",
        "for i, (enc_col, conf_col) in enumerate(zip(encoded_cols, confidence_cols)):\n",
        "    zero_confidence_mask = merged_df[conf_col] == 0\n",
        "    if zero_confidence_mask.any():\n",
        "        print(f\"Found {zero_confidence_mask.sum()} zero confidence scores for {enc_col}\")\n",
        "        encoded_df.loc[zero_confidence_mask, enc_col] = pd.NA  # Use pd.NA, not np.nan\n",
        "    # Ensure column is Int64 after assignment\n",
        "    encoded_df[enc_col] = encoded_df[enc_col].astype(\"Int64\")\n",
        "\n",
        "print(f\"Calculating Krippendorff's alpha for {len(encoded_cols)} encoded columns\")\n",
        "print(f\"Number of texts analysed: {len(encoded_df)}\")\n",
        "\n",
        "alpha = calculate_krippendorff_alpha(encoded_df, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d274941-c541-4174-beb0-69e2bc718d58",
      "metadata": {
        "id": "3d274941-c541-4174-beb0-69e2bc718d58"
      },
      "outputs": [],
      "source": [
        "encoded_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18fc8257-66a2-4400-bb31-5fd1ec483e77",
      "metadata": {
        "id": "18fc8257-66a2-4400-bb31-5fd1ec483e77"
      },
      "outputs": [],
      "source": [
        "encoded_df.to_csv(\"encoded_annotations.csv\", header=False, index=False, na_rep=\"NA\")\n",
        "print(\"Saved encoded values to encoded_annotations.csv (no headers, no index)\\nDownload this file and share with the class for testing with online K-Alpha calculator\\nPlease test first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65db0f11-796c-4222-bba2-98f7c7fac862",
      "metadata": {
        "id": "65db0f11-796c-4222-bba2-98f7c7fac862"
      },
      "outputs": [],
      "source": [
        "# Now get a single annotation per tweet for the class based on the mode for each tweet\n",
        "row_modes = encoded_df.mode(axis=1, dropna=True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ce65c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare the class mode label to the original dataset label\n",
        "orig_labels = pd.read_csv(\"/srv/source-data/tweets_original_labels.csv\", header = 0)\n",
        "row_modes_df = row_modes.to_frame().reset_index(drop=True)\n",
        "\n",
        "# Concatenate horizontally\n",
        "combined_class_orig = pd.concat([row_modes_df, orig_labels], axis=1)\n",
        "combined_class_orig = combined_class_orig.rename(columns={0: \"class_label\", \"label\": \"orig_label\"})\n",
        "\n",
        "combined_class_orig_enc = combined_class_orig.drop(columns=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b0a2331",
      "metadata": {},
      "outputs": [],
      "source": [
        "alpha_2 = calculate_krippendorff_alpha(combined_class_orig_enc, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa4dcbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_class_orig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7875c50e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save without header or index\n",
        "combined_class_orig_enc.to_csv(\"row_modes_and_orig_labels.csv\", header=False, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
